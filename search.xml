<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>留学生Computer Science编程作业代写、留学生CS辅导、CS编写</title>
    <url>/posts/35542.html</url>
    <content><![CDATA[<p><strong>个人介绍：</strong></p>
<p>我本科毕业于浙江大学计算机专业，之后在南加州大学攻读硕士学位，专注于深度学习、自然语言处理、全栈开发以及计算机网络和操作系统等领域。凭借多年的实践经验，我精通多种主流编程语言（如Python、Java、C++、JavaScript、Go）和框架（如TensorFlow、PyTorch、React、Node.js等），在大规模模型训练、分布式系统设计及全栈开发等方面具备深厚的技术积累。我曾在谷歌AI实验室担任研究实习生，参与多个重要项目，包括大规模分布式训练平台和低资源语言模型优化等。</p>
<p>与中介、机构或工作室的写手不同，所有编程作业均由我亲自完成。深知各类作业的得分要点和代码规范，我提供高质量的定制服务，确保每一份作业满足课程要求，并帮助您提高成绩。无论是在深度学习、游戏设计、操作系统、数据库系统还是Web设计等方面，我都能够提供专业辅导与作业代写服务。</p>
<p><strong>服务内容：</strong></p>
<ul>
<li>代写和辅导计算机科学领域的作业，涵盖Assignment、Homework、Lab、Project和Final，适用于北美（美国、加拿大）、澳洲、英国等地区的课程。</li>
<li>我的编程风格贴合您的课堂讲义和作业要求，避免复杂难懂的代码，所有代码与课堂内容完全契合，确保易于理解和实现。</li>
<li>擅长解决各类疑难作业，尤其在Game Design、Operating System、Database System、Web Design等领域有着广泛经验，成绩几乎全A。</li>
</ul>
<p><strong>质量保证：</strong></p>
<ul>
<li>提供带测试的完整代码，确保作业得分点完整覆盖，Bonus任务也会尽量完成。</li>
<li>小作业通常在24小时内完成，大作业在24-72小时内交付，急单12小时内完成，确保不浪费您的slip day。</li>
<li>所有代码均为我手写原创，严格遵守Honor Code，确保每份作业的代码都是独一无二。</li>
<li>免费提供售后服务，包括代码运行调试和讲解答疑，帮助您全面理解代码。</li>
<li>我的编码风格符合Google开源项目的标准，确保您在完成作业的同时，也能够提高自己的编程规范。</li>
</ul>
<p><strong>价格优势：</strong></p>
<ul>
<li>无中介费用，价格透明，支持PayPal、支付宝、微信支付等多种支付方式。</li>
<li>提供的不仅是作业答案，还有我作为算法工程师的经验与编程技巧，帮助您提升编程能力。</li>
</ul>
<h2><span id="lian-xi-fang-shi">联系方式</span><a href="#lian-xi-fang-shi" class="header-anchor">#</a></h2><p>为了节省时间，方便报价，询问时麻烦附上年级和课程名，如：<br>大二，Algorithm and Data Structure<br>作业要求请加微信：xqxsssj</p>
]]></content>
      <tags>
        <tag>导航</tag>
      </tags>
  </entry>
  <entry>
    <title>CS 457/557 Assignment 4 Due December 11 2024 by 11:00 pm (Central)</title>
    <url>/posts/48125.html</url>
    <content><![CDATA[<p>Overview</p>
<p>CS 457&#x2F;557: Assignment 4 Due: December 11, 2024, by 11:00 pm (Central)</p>
<p>In this assignment, you will be implementing the SARSA and Q-learning algorithms for a navigation task in a simple grid world environment.</p>
<p>Academic Integrity Policy</p>
<p>All work for this assignment must be your own work and it must be completed independently. Using code from other students and&#x2F;or online sources (e.g., Github) constitutes academic miscon- duct, as does sharing your code with others either directly or indirectly (e.g., by posting it online). Academic misconduct is a violation of the UWL Student Honor Code and is unacceptable. Pla- giarism or cheating in any form may result in a zero on this assignment, a negative score on this assignment, failure of the course, and&#x2F;or additional sanctions. Refer to the course syllabus for additional details on academic misconduct.</p>
<p>You should be able to complete the assignment using only the course notes and textbook along with relevant programming language documentation (e.g., the Java API specification). Use of additional resources is discouraged but not prohibited, provided that this is limited to high-level queries and not assignment-specific concepts. As a concrete example, searching for “how to use a HashMap in Java” is fine, but searching for “Q-learning in Java” is not.</p>
<p>Deliverables</p>
<p>You should submit a single compressed archive (either .zip or .tgz format) containing the following to Canvas:</p>
<ol>
<li>The complete source code for your program. I prefer that you use Java in your implemen- tation, but if you would like to use another language, check with me before you get started. Additional source code requirements are listed below:<ul>
<li>Your name must be included in a header comment at the top of each source code file.</li>
<li>Your code should follow proper software engineering principles for the chosen language, including meaningful comments and appropriate code style.</li>
<li>Your code must not make use of any non-standard or third-party libraries.</li>
</ul>
</li>
<li>A README text file that provides instructions for how your program can be compiled (as needed) and run from the command line. If your program is incomplete, then your README should document what parts of the program are and are not working.<span id="more"></span></li>
</ol>
<p>Program Requirements</p>
<p>The general outline of the program is given below:</p>
<p>Algorithm 1 Program Flow</p>
<p>Process command-line arguments<br> Load environment from the specified file<br> Perform learning episodes, with periodic parameter updates and&#x2F;or evaluation Perform final evaluation of pure greedy policy based on learned Q values Print final learned policy and additional details as appropriate</p>
<p>The program should be runnable from the command line, and it should be able to process command- line arguments to update various program parameters as needed. The command-line arguments are listed below:</p>
<ul>
<li><p>-f <filename>: Reads the environment from the file named <filename> (specified as a String); see the File Format section for more details.</filename></filename></p>
</li>
<li><p>-a <double>: Specifies the (initial) learning rate (step size) α ∈ [0, 1]; default is 0.9.</double></p>
</li>
<li><p>-e <double>: Specifies the (initial) policy randomness value ε ∈ [0, 1]; default is 0.9.</double></p>
</li>
<li><p>-g <double>: Specifies the discount rate γ ∈ [0, 1] to use during learning; default is 0.9.</double></p>
</li>
<li><p>-na <integer>: Specifies the value Nα which controls the decay of the learning rate (step</integer></p>
<p>size) α; default is 1000.</p>
</li>
<li><p>-ne <integer>: Specifies the value Nε which controls the decay of the random action thresh-</integer></p>
<p>old ε; default is 200.</p>
</li>
<li><p>-p <double>: Specifies the action success probability p ∈ [0, 1]; default is 0.8.</double></p>
</li>
<li><p>-q: Toggles the use of Q-Learning with off-policy updates (instead of SARSA with on-policy</p>
<p>updates, which is the default).</p>
</li>
<li><p>-T <integer>: Specifies the number of learning episodes (trials) to perform; default is 10000.</integer></p>
</li>
<li><p>-u: Toggles the use of Unicode characters in printing; disabled by default (see the Output</p>
<p>section for details).</p>
</li>
<li><p>-v <integer>: Specifies a verbosity level, indicating how much output the program should</integer></p>
<p>produce; default is 1 (See the Output section for details)</p>
<p>The -f <filename> option is required; all others are optional. You can assume that your program will only be run with valid arguments (so you do not need to include error checking, though it may be helpful for your own testing). Your program must be able to handle command-line arguments in any order (e.g., do not assume that the first argument will be -f). Several example runs of the program are shown at the end of this document.</filename></p>
<p>Extra credit: Any program that includes support for one or both of the following options will be eligible for a small amount of extra credit (see page 10 for details):</p>
<ul>
<li>-l <double>: Specifies the λ parameter for eligibility trace decay; default is 0.0 (meaning that eligibility traces should not be used by default).</double></li>
<li>-w: Specifies that the agent should use a weighted sum of features to estimate Q values for each state-action pair instead of maintaining these values in a table; disabled by default.</li>
</ul>
</li>
</ul>
<p>Environment Details</p>
<p>The environment in which the agent operates consists of a rectangular grid of cells, with each cell belonging to one of the following types:</p>
<ul>
<li><p>S indicates the start cell for an agent</p>
</li>
<li><p>G indicates a goal cell</p>
</li>
<li><p>indicates an empty cell</p>
</li>
<li><p>B indicates a block cell</p>
</li>
<li><p>M indicates an explosive mine cell</p>
</li>
<li><p>C indicates a cliff cell</p>
<p>In an environment, the agent begins in the start cell (S), and can choose to move in one of four directions (up, down, left, and right). Goal (G) and mine (M) cells are terminal states, which means no further action is possible upon reaching those states. Cliff (C) cells are “restart” cells: any action taken in a cliff cell causes the agent to return to the start cell for the next step. Block (B) cells are obstacles that the agent must circumvent (i.e., the agent cannot enter a block cell).</p>
<p>In all other types of cells, any attempt at movement either succeeds as intended with some proba- bility p ∈ [0, 1] or results in movement plus drift with probability 1 − p. The drift is perpendicular to the intended direction of movement, and each drift direction is equally likely. For example, in Figure 1 below, the agent is in the cell marked X and is attempting to move up to the cell marked T. The following outcomes are possible:</p>
<p>• the agent moves up to enter cell T as intended with probability p;<br> • the agent moves up and drifts left to enter cell L with probability (1 − p)&#x2F;2; • the agent moves up and drifts right to enter cell R with probability (1 − p)&#x2F;2.</p>
<p>Any movement (including movement plus drift) that would cause the agent to leave the bounds of the grid or enter a block cell (B) will fail and result in the agent staying in its current location. For example, in Figure 2 below, the agent is at the right edge of the environment and is attempting to move up to the cell marked T. The following outcomes are possible:</p>
</li>
<li><p>the agent moves up to enter cell T as intended with probability p;</p>
</li>
<li><p>the agent moves up and drifts left to enter cell L with probability (1 − p)&#x2F;2;</p>
</li>
<li><p>the agent agent remains in place with probability (1 − p)&#x2F;2 because moving up and drifting</p>
<p>right would take the agent out of bounds.</p>
<p>Figure 3 below shows how block cells influence outcomes. The agent is again in the cell marked X and attempting to move up. The only movement that can occur is moving up and drifting left (with probability (1−p)&#x2F;2); moving up without drift and moving up with rightward drift both lead to block cells which the agent cannot enter, so these outcomes result in no movement. (Note that the possibility of drift allows the agent to move on the diagonal in an indirect way; this can lead to some interesting “wall-hack” behavior that allows the agent to squeeze through gaps between two block cells that are adjacent to the agent’s current location.)</p>
</li>
</ul>
<p>As the agent moves, it receives rewards as follows:</p>
<p>• Entering a goal cell yields a reward of +10.<br> • Entering a mine cell yields a reward of −100.<br> • Entering a cliff cell yields a reward of −20.<br> • Acting in a cliff cell and returning to the start yields a reward of −10. • Any other result yields a reward of −1.</p>
<p>File Format</p>
<p>The environment for the agent will be specified in a text file containing cell markers (S, G, , B, M, C) as described earlier. Empty lines and lines that begin with # should be ignored. A simple 2 × 5 grid world is shown below:</p>
<p>M_BG_ CS__C</p>
<p>Several example input files are provided in the a04-data.zip archive on Canvas.</p>
<p>Basic Reinforcement Learning</p>
<p>The basic learning agent will use either the SARSA or Q-Learning algorithms to learn Q values for all state-action pairs via temporal difference updates. Your basic learning agent will need to maintain Q(s,a) values for all possible state-action pairs (s,a), where s is one of the cells in the grid world and a is one of the four movement directions (up, down, left, right).</p>
<p>To learn, the agent should perform T learning episodes (trials), where the value of T is either specified via the command-line flag -T or the default of 10,000. In each learning episode (trial), the agent begins in the start cell of the environment. The agent should then select and execute actions according to an ε-greedy policy and update its Q values using either on-policy (SARSA) or off-policy (Q-Learning) updates as appropriate. The agent should continue acting until it reaches a terminal state (goal or mine cell) or it takes a number of actions equal to the entire size (width × height) of the environment, at which point the episode ends.</p>
<p>Parameters and Decay</p>
<p>Initially, the agent should use the control parameters α, γ, and ε with values set to their defaults or specified via corresponding command-line arguments. As learning proceeds, the agent should periodically update its α and ε parameters to allow the Q values to converge and to direct the agent towards a pure greedy policy. Let α(0) and ε(0) denote the initial values of α and ε, and assume that the learning episodes performed by the agent are labeled 1, 2, . . . , T . In episode t, the agent should use α(t) and ε(t) values determined as</p>
<p> where Nα and Nε are (integer-valued) hyperparameters that control the decay of the learning rate</p>
<p>and policy randomness, respectively, and ⌈·⌉ is the ceiling function.<br> By design, the α(t) values will only change after Nα episodes have completed (similarly for the</p>
<p>ε(t) values). For example, we have</p>
<p>α(0) if t ∈ {1,2,…,Nα},<br> α(t) &#x3D;α(0)&#x2F;2 ift∈{Nα+1,Nα+2,…,2Nα},</p>
<p>α(0)&#x2F;3 ift∈{2Nα+1,…,3Nα}</p>
<p>CS 457&#x2F;557 Assignment 4 Page 5 of 12</p>
<p>This means that the agent only needs to update its α value after completing an episode that is a multiple of Nα (similarly for ε). With a little algebra, we can derive the following update rules, where t is the number of the episode that was just completed:</p>
<p>α(0)<br> • After every Nα episodes, update α to 1 + ⌊t&#x2F;Nα⌋.</p>
<p>ε(0)<br> • After every Nε episodes, update ε to 1 + ⌊t&#x2F;Nε⌋.</p>
<p>The periodic update rules above avoid having to recalculate α(t) and ε(t) at the start of each episode, and they also simplify the calculations somewhat (Hint: you can use integer division in Java to compute the floor function).</p>
<p>Evaluation of Final Learned Policy</p>
<p>After all learning episodes have completed, the agent should evaluate the performance of a pure greedy policy based on its learned Q values. To do this, the agent should perform 50 evaluation episodes; in each evaluation episode, the agent should always act greedily based on its Q values, and it should not update its Q values during evaluation. At the end of each evaluation episode, the agent should record the total reward (without discounting) that it earned by following the greedy policy. The performance of the greedy policy is then estimated as the average reward earned across the 50 evaluation episodes.</p>
<p>(Hint: There are only a few differences between learning episodes and evaluation episodes, so you should be able to write common code for performing an episode of either type, and customize behavior based on one or two conditional flags.)</p>
<p>Output</p>
<p>The level of output produced by the program is controlled by the -v flag (for verbosity). The levels range from 1 to 4, with 4 containing the most output; the output produced is additive, meaning that the output for level i should also be produced for any level j with j ≥ i. The desired output for each level is discussed in more detail below.</p>
<p>Output Level 1<br> Level 1 prints a brief summary of the program flow followed by details of the learned policy:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-example.txt -v 1 * Reading data&#x2F;simple-example.txt…<br> * Beginning 10000 learning episodes with SARSA…</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.500</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">MvBG&lt;</span><br><span class="line">&lt;&gt;&gt;ˆ&lt;</span><br></pre></td></tr></table></figure>

<p>Printing the policy involves displaying the best action (or actions in the case of ties) in each non- terminal and non-block state, where best is determined by the Q values, while displaying the cell marker for M, G, and B states. If you have a console that supports unicode output, then you can display the policy using unicode characters with the -u flag. The various combinations of best action and corresponding printed character (ASCII or Unicode) are shown below:</p>
<p>CS 457&#x2F;557</p>
<p>Assignment 4</p>
<p>Page 6 of 12</p>
<p>Best Action Set { Left }</p>
<p>{ Up } { Right } {Down}</p>
<p>{ Left, Right } { Up, Down } { Up, Left }<br> { Up, Right } { Down, Right } { Down, Left }</p>
<p>{ Up, Down, Right } { Up, Down, Left } { Down, Left, Right } { Up, Left, Right }</p>
<p>ASCII Symbol</p>
<p>&lt; ← ^ ↑ &gt; → v ↓</p>
<p>- ↔ | ↕ \ ↖ &#x2F; ↗ \ ↘ &#x2F; ↙</p>
<p>&gt; ⊢ &lt; ⊣ v ⊤ ^ ⊥</p>
<p>Unicode<br> Java character</p>
<p>\u2190 \u2191 \u2192 \u2193</p>
<p>\u2194 \u2195 \u2196 \u2197 \u2198 \u2199</p>
<p>\u22a2 \u22a3 \u22a4 \u22a5</p>
<p>{ Up, Down, Left, Right }<br> You can print a unicode character in Java using System.out.print(“\u2190”); or similar. An</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-f data/simple-minefield.txt -q -u -p 1</span><br></pre></td></tr></table></figure>

<p>is shown below:</p>
<p>Embedding unicode output in this pdf document directly is non-trivial, so all remaining examples here will use ASCII output.</p>
<p>+ (just use ASCII)<br> image of unicode output on the console, produced with the command-line arguments</p>
<p>CS 457&#x2F;557 Assignment 4 Page 7 of 12</p>
<p>Output Level 2<br> Level 2 includes the final learned Q values in the output:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-example.txt -v 2 -q * Reading data&#x2F;simple-example.txt…<br> * Beginning 10000 learning episodes with Q-Learning…</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.620</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">MvBG&lt;</span><br><span class="line">ˆ&gt;&gt;ˆv</span><br><span class="line">* Learned Q values:</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">|</span><br><span class="line">|0.0</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">0.0  |    -4.4  |</span><br><span class="line">     |-92.5     |0.0</span><br></pre></td></tr></table></figure>

<p>0.0 | |0.0</p>
<p>0.0 | |9.8</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  0.0|      -1.8|</span><br><span class="line">0.0  |     0.4  |</span><br></pre></td></tr></table></figure>

<p>0.0| 0.0 |</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">           7.3  |</span><br><span class="line">                |</span><br><span class="line">             7.4|</span><br><span class="line">0.0  |   -23.5  |</span><br></pre></td></tr></table></figure>

<p>0.0|</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|    -5.1  |   -18.8  |     5.5  |</span><br><span class="line">|-5.3      |-33.9     |4.1       |6.1</span><br><span class="line">|      -5.4|       4.9|       7.3|</span><br><span class="line">|    -5.6  |     4.0  |     5.8  |</span><br><span class="line">--------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>Each cell (state) in the environment is displayed in the above output, along with the four Q values associated with the different actions in that state. Note that terminal cells and blocks should have all of their Q values set to 0, because the agent never acts in such cells. Each cell has an internal width of 10 characters (which excludes the borders). Each Q value is displayed with a width of 6 characters (leading zeroes are suppressed) and one decimal point (format specifier %6.1f) and the Q values are arranged as follows:</p>
<p>• Q(s,Up) and Q(s,Down) are centered at the top and bottom of the cell, respectively, each with a minimum of two characters of blank space on either side;</p>
<p>• Q(s, Left) is flushed left on the second line in the cell (use format specifier %-6.1f here); • Q(s, Right) is flushed right on the third line in the cell.</p>
<p>Your output doesn’t have to match this exactly, but it should be displayed in a clean and readable format; in particular, strive to ensure that the cells are displayed with a fixed width.</p>
<p>Output Level 3</p>
<p>Level 3 will periodically evaluate a pure greedy policy based on the current Q values at various stages during the learning process. Specifically, after every T&#x2F;10 learning episodes, the agent should perform 50 evaluation episodes and record the average total reward that is earned across these evaluation episodes. These evaluation episodes should operate as described earlier in the Evaluation of Final Learned Policy section. (Note that at this output level, it is fine to have your program evaluate the last greedy policy twice, once after the last learning episode and once at the end for final evaluation; depending on uncertainty within the environment, you might get different average total reward estimates for these separate evaluations.)</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simpler-example.txt -v 3 -T 1200 * Reading data&#x2F;simpler-example.txt…</p>
<p>9.4 | |-5.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-5.0  |</span><br><span class="line">      |</span><br><span class="line">  -5.2|</span><br><span class="line">-4.9  |</span><br></pre></td></tr></table></figure>

<p>-20.2| 7.6 |</p>
<p>CS 457&#x2F;557 Assignment 4 Page 8 of 12</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">* Beginning 1200 learning episodes with SARSA...</span><br><span class="line">* After</span><br><span class="line">* Episode</span><br><span class="line">      120</span><br><span class="line">      240</span><br><span class="line">      360</span><br><span class="line">      480</span><br><span class="line">      600</span><br><span class="line">      720</span><br><span class="line">      840</span><br><span class="line">      960</span><br><span class="line">     1080</span><br><span class="line">     1200</span><br><span class="line">Avg. Total Reward for</span><br><span class="line">Current Greedy Policy</span><br><span class="line">  -8.000</span><br><span class="line">   3.580</span><br><span class="line">  -8.000</span><br><span class="line">  -8.000</span><br><span class="line">   3.020</span><br><span class="line">  -8.000</span><br><span class="line">   4.780</span><br><span class="line">  -6.860</span><br><span class="line">   6.760</span><br><span class="line">   6.960</span><br><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.060</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">v&gt;&gt;G</span><br><span class="line">&gt;&gt;&gt;ˆ</span><br><span class="line">* Learned Q values:</span><br><span class="line">---------------------------------------------</span><br><span class="line">                                       0.0  |</span><br><span class="line">                                            |</span><br><span class="line">                                         0.0|</span><br><span class="line">                                       0.0  |</span><br><span class="line">---------------------------------------------</span><br></pre></td></tr></table></figure>

<p>| -3.1 |</p>
<p>0.3 | |1.4</p>
<p>5.7 | |0.0</p>
<p>|-3.9 |<br> |</p>
<p>|-2.2 -4.8|</p>
<p>7.1| 0.3 |</p>
<p>10.0| 5.1 |</p>
<p>1.2 |</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|</span><br><span class="line">|1.2</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">---------------------------------------------</span><br></pre></td></tr></table></figure>

<p>Output Level 4<br> Level 4 displays the changes made to the α and ε parameters during the learning process:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-example.txt -v 4 -q -T 200 -na 100 -ne 50 * Reading data&#x2F;simple-example.txt…<br> * Beginning 200 learning episodes with Q-Learning…</p>
<p>-0.4 | |-0.2</p>
<p>3.4 | |0.2</p>
<p>3.4 | |4.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">9.7  |</span><br><span class="line">     |</span><br><span class="line">  7.5|</span><br><span class="line">6.9  |</span><br></pre></td></tr></table></figure>

<p>4.7| 0.7 |</p>
<p>7.1| 3.3 |</p>
<p>7.8| 4.1 |</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">* After</span><br><span class="line">* Episode</span><br><span class="line">       20</span><br><span class="line">       40</span><br><span class="line">Avg. Total Reward for</span><br><span class="line">Current Greedy Policy</span><br></pre></td></tr></table></figure>

<p>-4.720 7.340</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(after episode 50, epsilon to 0.45000)</span><br></pre></td></tr></table></figure>

<p>60</p>
<p>80 100</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  7.620</span><br><span class="line">-10.000</span><br><span class="line">  0.340</span><br><span class="line">(after episode 100, alpha to 0.45000)</span><br><span class="line">(after episode 100, epsilon to 0.30000)</span><br></pre></td></tr></table></figure>

<p>120 7.440</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    140      7.400</span><br><span class="line">(after episode 150, epsilon to 0.22500)</span><br><span class="line">    160      7.440</span><br><span class="line">    180      7.260</span><br><span class="line">    200      7.700</span><br></pre></td></tr></table></figure>

<p>CS 457&#x2F;557 Assignment 4 Page 9 of 12</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    (after episode 200, alpha to 0.30000)</span><br><span class="line">    (after episode 200, epsilon to 0.18000)</span><br><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.560</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">MvBG&lt;</span><br><span class="line">ˆ&gt;&gt;ˆ&gt;</span><br><span class="line">* Learned Q values:</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">|     0.0  |    -1.9  |</span><br><span class="line">|0.0       |-100.0    |0.0</span><br><span class="line">|       0.0|      -1.4|</span><br><span class="line">|     0.0  |     4.3  |</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">|    -3.7  |   -58.8  |     5.9  |</span><br><span class="line">|-7.1      |-52.6     |4.1       |6.3</span><br><span class="line">|      -4.2|       6.0|       7.4|</span><br><span class="line">|    -5.1  |     4.5  |     6.5  |</span><br><span class="line">--------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>0.0 | |0.0</p>
<p>0.0 | |9.8</p>
<p>0.0| 0.0 |</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">           2.6  |</span><br><span class="line">                |</span><br><span class="line">             6.5|</span><br><span class="line">0.0  |   -22.2  |</span><br></pre></td></tr></table></figure>

<p>0.0|</p>
<p>9.4 | |-4.7</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-5.0  |</span><br><span class="line">      |</span><br><span class="line">  -4.6|</span><br><span class="line">-5.1  |</span><br></pre></td></tr></table></figure>

<p>-18.9| 7.5 |</p>
<p>CS 457&#x2F;557 Assignment 4 Page 10 of 12</p>
<p>Extra Credit Options</p>
<p>Eligibility Traces</p>
<p>The optional -l command-line argument allows for specification of the λ parameter to be used in conjunction with eligibility traces. For λ &#x3D; 0, the learning agent should perform temporal difference updates as before (i.e., only one Q(s, a) value should be updated at each step in a learning episode).</p>
<p>For λ &gt; 0, the agent will need to store an eligibility value e(s, a) for each (s, a) ∈ S × A. Whenever the agent takes action a in state s, e(s, a) should go up by 1. After each step in a learning episode, the agent should update the Q and e values for each state-action pair (s′′, a′′) ∈ S × A using:</p>
<p>Q(s′′, a′′) ← Q(s′′, a′′) + αδe(s′′, a′′), e(s′′, a′′) ← γλe(s′′, a′′),</p>
<p>where δ is the TD error at the current step. Some additional notes:</p>
<p>• The eligibility values should be reset back to 0 at the start of each episode.</p>
<p>• Using eligibility traces in conjunction with SARSA is straightforward, but using traces in Q-Learning requires more care. Specifically, in Q-Learning, the eligibility values e(s, a) need to be reset back to 0 whenever the next action is sub-optimal.</p>
<p>See Lecture 09-3 for further details.<br> The example below uses SARSA with 100 learning episodes and deterministic movement (i.e., every</p>
<p>action succeeds as intended), without eligibility traces:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-cliff.txt -p 1 -T 100 -l 0.0 * Reading data&#x2F;simple-cliff.txt…<br> * Beginning 100 learning episodes with SARSA…</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: -8.000</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">&lt;v&gt;G</span><br><span class="line">v&gt;&gt;ˆ</span><br></pre></td></tr></table></figure>

<p>When eligibility traces are included, learning improves:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-cliff.txt -p 1 -T 100 -l 0.3 * Reading data&#x2F;simple-cliff.txt…<br> * Beginning 100 learning episodes with SARSA…</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.000</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">v&lt;&lt;G</span><br><span class="line">&gt;&gt;&gt;ˆ</span><br></pre></td></tr></table></figure>

<p>CS 457&#x2F;557 Assignment 4 Page 11 of 12</p>
<p>Feature-Based Q-Learning</p>
<p>The optional -w command-line argument should toggle the use of feature-based Q-learning. Instead of maintaining Q values for each state-action pair in a table, the agent should maintain a set of weights associated with features that characterize the state-action pairs.</p>
<p>For any state s, we will define the following features:</p>
<p>f0(s) &#x3D; 1<br> f1(s) &#x3D; x coordinate of s</p>
<p>xmax<br> f2(s) &#x3D; y coordinate of s</p>
<p>ymax<br> f3(s) &#x3D; 1 if s is a mine cell, 0 otherwise</p>
<p>f4(s) &#x3D; 1 if s is a cliff cell, 0 otherwise f5(s) &#x3D; 1 if s is a goal cell, 0 otherwise f6(s) &#x3D; L1 distance from s to nearest goal</p>
<p>xmax + ymax</p>
<p>where we assume that the x and y coordinates of states are positive integers, (1, 1) is the state in the lower left corner of the environment, and (xmax,ymax) is the state in the upper right corner of the environment; i.e., state coordinates look like:</p>
<p>(xmax, ymax)</p>
<p>For any state-action pair (s, a), we will define the following features, most of which are based on the features of the likely next state s′ that would result from taking action a in state s (i.e., s′ is the state that the agent would reach if it did not drift):</p>
<p>fj(s,a) &#x3D; fj(s′) for j ∈ {0,1,…,6} f7(s,a) &#x3D; 1 if s &#x3D; s′, 0 otherwise</p>
<p>The agent should maintain a weight wj for each state-action feature fj, and it should estimate the Q value for any (s, a) pair as</p>
<p>7</p>
<p>Q(s,a)&#x3D;Xwj ·fj(s,a) j&#x3D;0</p>
<p>These weights should be initialized to 1 and then updated during learning episodes as described in Lecture 09-4, where δ is the TD error:</p>
<p>wj←wj+αδfj(s,a) ∀j∈{0,1,2,…,7}</p>
<p>For verbosity level 2 and up, your program should print out the final learned weights after the Q values (note that these Q values should be the estimated values based on the weights and state- action features). An example is shown on the next page.</p>
<p>(1, ymax)</p>
<p>.</p>
<p>(1, 1)</p>
<p>(2, 1)</p>
<p>…</p>
<p>(xmax, 1)</p>
<p>CS 457&#x2F;557 Assignment 4 Page 12 of 12</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-cliff.txt -q -p 1 -T 50 -v 2 -w * Reading data&#x2F;simple-cliff.txt…<br> * Beginning 50 learning episodes with Q-Learning…</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.000</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">&gt;v+G</span><br><span class="line">&gt;&gt;&gt;ˆ</span><br><span class="line">* Learned Q values:</span><br><span class="line">---------------------------------------------</span><br><span class="line">|    50.3  |    58.5  |    57.2  |    95.3  |</span><br><span class="line">|50.3      |50.6      |57.2      |32.3      |</span><br><span class="line">|      58.9|      32.3|      57.2|</span><br><span class="line">|    57.2  |    65.4  |    57.2  |</span><br><span class="line">---------------------------------------------</span><br><span class="line">|    50.6  |    58.9  |    32.3  |    95.7  |</span><br><span class="line">|56.9      |57.2      |65.4      |73.7      |</span><br></pre></td></tr></table></figure>

<p>95.3| 81.9 |</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|      65.4|      73.7|      81.9|</span><br><span class="line">|    56.9  |    65.1  |    73.3  |</span><br><span class="line">---------------------------------------------</span><br><span class="line">* Learned weights</span><br><span class="line">  Feature 0 (constant):  35.526</span><br><span class="line">  Feature 1 (scaled-X):  33.358</span><br><span class="line">  Feature 2 (scaled-Y):  12.902</span><br><span class="line">  Feature 3 (   ind-M):   1.000</span><br><span class="line">  Feature 4 (   ind-C): -34.795</span><br><span class="line">  Feature 5 (   ind-G):  20.321</span><br><span class="line">  Feature 6 (  dist-G):   0.658</span><br><span class="line">  Feature 7 (ind-move):  -0.339</span><br></pre></td></tr></table></figure>

<p>Some final notes:</p>
<ul>
<li>In my limited experimentation with feature-based learning, the learning process seems a bit more temperamental with regards to the various control parameters (e.g., α, ε, decay rates). Q-Learning with features tends to do better than SARSA with features, but you can probably get SARSA to behave well by adjusting the control parameters appropriately (e.g., extending the learning process, adjusting ε decay).</li>
<li>If you’re feeling adventurous, you can try to devise some additional features to use and see if they work better than what is given here. You could also try mixing and matching the features, e.g., perhaps by removing the coordinate-based features and replacing them with something else. Let me know if you find any features that seem to work well!</li>
<li>You can also add eligibility traces to feature-based learning (i.e., support both the -l and -w flags at the same time). This requires a few modifications to the eligibility values and the weight update process, though. You can read up on this in the RL textbook and&#x2F;or stop by office hours if you’re curious to know how this might be done.</li>
</ul>
<p>81.6| 81.6 |</p>
]]></content>
      <categories>
        <category>CS 457/557</category>
      </categories>
      <tags>
        <tag>CS 457/557 Assignment 4 Due December 11 2024 by 11:00 pm (Central)</tag>
      </tags>
  </entry>
</search>
