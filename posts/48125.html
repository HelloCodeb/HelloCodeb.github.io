<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">



<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"javamianshi.vercel.app","root":"/","scheme":"Gemini","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":false,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>



  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "bb467ce9"
    });
  daovoice('update');
  </script>

  <meta name="description" content="All work for this assignment must be your own work and it must be completed independently. Using code from other students and&#x2F;or online sources (e.g., Github) constitutes academic miscon- duct, as doe">
<meta property="og:type" content="article">
<meta property="og:title" content="CS 457&#x2F;557 Assignment 4 Due December 11 2024 by 11:00 pm (Central)">
<meta property="og:url" content="https://javamianshi.vercel.app/posts/48125.html">
<meta property="og:site_name" content="留学生CS课业辅导">
<meta property="og:description" content="All work for this assignment must be your own work and it must be completed independently. Using code from other students and&#x2F;or online sources (e.g., Github) constitutes academic miscon- duct, as doe">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-01-11T13:41:11.000Z">
<meta property="article:modified_time" content="2025-01-11T14:25:11.699Z">
<meta property="article:author" content="微信扫码留学生CS作业">
<meta property="article:tag" content="CS 457&#x2F;557 Assignment 4 Due December 11 2024 by 11:00 pm (Central)">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://javamianshi.vercel.app/posts/48125.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css"><style>
#needsharebutton-postbottom {
  cursor: pointer;
  height: 26px;
  margin-top: 10px;
  position: relative;
}
#needsharebutton-postbottom .btn {
  border: 1px solid $btn-default-border-color;
  border-radius: 3px;
  display: initial;
  padding: 1px 4px;
}
</style>
  <title>CS 457/557 Assignment 4 Due December 11 2024 by 11:00 pm (Central) | 留学生CS课业辅导</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="留学生CS课业辅导" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">留学生CS课业辅导</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">1v1留学生CS作业代写，计算机课业辅导，请加V：xqxsssj</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://javamianshi.vercel.app/posts/48125.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avator.jpg">
      <meta itemprop="name" content="微信扫码留学生CS作业">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="留学生CS课业辅导">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS 457/557 Assignment 4 Due December 11 2024 by 11:00 pm (Central)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-01-11 21:41:11 / 修改时间：22:25:11" itemprop="dateCreated datePublished" datetime="2025-01-11T21:41:11+08:00">2025-01-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CS-457-557/" itemprop="url" rel="index"><span itemprop="name">CS 457/557</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>26k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>23 分钟</span>
            </span>
            <div class="post-description">All work for this assignment must be your own work and it must be completed independently. Using code from other students and/or online sources (e.g., Github) constitutes academic miscon- duct, as does sharing your code with others either directly or indirectly (e.g., by posting it online). Academic misconduct is a violation of the UWL Student Honor Code and is unacceptable. Pla- giarism or cheating in any form may result in a zero on this assignment, a negative score on this assignment, failure of the course, and/or additional sanctions. Refer to the course syllabus for additional details on academic misconduct</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Overview</p>
<p>CS 457&#x2F;557: Assignment 4 Due: December 11, 2024, by 11:00 pm (Central)</p>
<p>In this assignment, you will be implementing the SARSA and Q-learning algorithms for a navigation task in a simple grid world environment.</p>
<p>Academic Integrity Policy</p>
<p>All work for this assignment must be your own work and it must be completed independently. Using code from other students and&#x2F;or online sources (e.g., Github) constitutes academic miscon- duct, as does sharing your code with others either directly or indirectly (e.g., by posting it online). Academic misconduct is a violation of the UWL Student Honor Code and is unacceptable. Pla- giarism or cheating in any form may result in a zero on this assignment, a negative score on this assignment, failure of the course, and&#x2F;or additional sanctions. Refer to the course syllabus for additional details on academic misconduct.</p>
<p>You should be able to complete the assignment using only the course notes and textbook along with relevant programming language documentation (e.g., the Java API specification). Use of additional resources is discouraged but not prohibited, provided that this is limited to high-level queries and not assignment-specific concepts. As a concrete example, searching for “how to use a HashMap in Java” is fine, but searching for “Q-learning in Java” is not.</p>
<p>Deliverables</p>
<p>You should submit a single compressed archive (either .zip or .tgz format) containing the following to Canvas:</p>
<ol>
<li>The complete source code for your program. I prefer that you use Java in your implemen- tation, but if you would like to use another language, check with me before you get started. Additional source code requirements are listed below:<ul>
<li>Your name must be included in a header comment at the top of each source code file.</li>
<li>Your code should follow proper software engineering principles for the chosen language, including meaningful comments and appropriate code style.</li>
<li>Your code must not make use of any non-standard or third-party libraries.</li>
</ul>
</li>
<li>A README text file that provides instructions for how your program can be compiled (as needed) and run from the command line. If your program is incomplete, then your README should document what parts of the program are and are not working.<span id="more"></span></li>
</ol>
<p>Program Requirements</p>
<p>The general outline of the program is given below:</p>
<p>Algorithm 1 Program Flow</p>
<p>Process command-line arguments<br> Load environment from the specified file<br> Perform learning episodes, with periodic parameter updates and&#x2F;or evaluation Perform final evaluation of pure greedy policy based on learned Q values Print final learned policy and additional details as appropriate</p>
<p>The program should be runnable from the command line, and it should be able to process command- line arguments to update various program parameters as needed. The command-line arguments are listed below:</p>
<ul>
<li><p>-f <filename>: Reads the environment from the file named <filename> (specified as a String); see the File Format section for more details.</filename></filename></p>
</li>
<li><p>-a <double>: Specifies the (initial) learning rate (step size) α ∈ [0, 1]; default is 0.9.</double></p>
</li>
<li><p>-e <double>: Specifies the (initial) policy randomness value ε ∈ [0, 1]; default is 0.9.</double></p>
</li>
<li><p>-g <double>: Specifies the discount rate γ ∈ [0, 1] to use during learning; default is 0.9.</double></p>
</li>
<li><p>-na <integer>: Specifies the value Nα which controls the decay of the learning rate (step</integer></p>
<p>size) α; default is 1000.</p>
</li>
<li><p>-ne <integer>: Specifies the value Nε which controls the decay of the random action thresh-</integer></p>
<p>old ε; default is 200.</p>
</li>
<li><p>-p <double>: Specifies the action success probability p ∈ [0, 1]; default is 0.8.</double></p>
</li>
<li><p>-q: Toggles the use of Q-Learning with off-policy updates (instead of SARSA with on-policy</p>
<p>updates, which is the default).</p>
</li>
<li><p>-T <integer>: Specifies the number of learning episodes (trials) to perform; default is 10000.</integer></p>
</li>
<li><p>-u: Toggles the use of Unicode characters in printing; disabled by default (see the Output</p>
<p>section for details).</p>
</li>
<li><p>-v <integer>: Specifies a verbosity level, indicating how much output the program should</integer></p>
<p>produce; default is 1 (See the Output section for details)</p>
<p>The -f <filename> option is required; all others are optional. You can assume that your program will only be run with valid arguments (so you do not need to include error checking, though it may be helpful for your own testing). Your program must be able to handle command-line arguments in any order (e.g., do not assume that the first argument will be -f). Several example runs of the program are shown at the end of this document.</filename></p>
<p>Extra credit: Any program that includes support for one or both of the following options will be eligible for a small amount of extra credit (see page 10 for details):</p>
<ul>
<li>-l <double>: Specifies the λ parameter for eligibility trace decay; default is 0.0 (meaning that eligibility traces should not be used by default).</double></li>
<li>-w: Specifies that the agent should use a weighted sum of features to estimate Q values for each state-action pair instead of maintaining these values in a table; disabled by default.</li>
</ul>
</li>
</ul>
<p>Environment Details</p>
<p>The environment in which the agent operates consists of a rectangular grid of cells, with each cell belonging to one of the following types:</p>
<ul>
<li><p>S indicates the start cell for an agent</p>
</li>
<li><p>G indicates a goal cell</p>
</li>
<li><p>indicates an empty cell</p>
</li>
<li><p>B indicates a block cell</p>
</li>
<li><p>M indicates an explosive mine cell</p>
</li>
<li><p>C indicates a cliff cell</p>
<p>In an environment, the agent begins in the start cell (S), and can choose to move in one of four directions (up, down, left, and right). Goal (G) and mine (M) cells are terminal states, which means no further action is possible upon reaching those states. Cliff (C) cells are “restart” cells: any action taken in a cliff cell causes the agent to return to the start cell for the next step. Block (B) cells are obstacles that the agent must circumvent (i.e., the agent cannot enter a block cell).</p>
<p>In all other types of cells, any attempt at movement either succeeds as intended with some proba- bility p ∈ [0, 1] or results in movement plus drift with probability 1 − p. The drift is perpendicular to the intended direction of movement, and each drift direction is equally likely. For example, in Figure 1 below, the agent is in the cell marked X and is attempting to move up to the cell marked T. The following outcomes are possible:</p>
<p>• the agent moves up to enter cell T as intended with probability p;<br> • the agent moves up and drifts left to enter cell L with probability (1 − p)&#x2F;2; • the agent moves up and drifts right to enter cell R with probability (1 − p)&#x2F;2.</p>
<p>Any movement (including movement plus drift) that would cause the agent to leave the bounds of the grid or enter a block cell (B) will fail and result in the agent staying in its current location. For example, in Figure 2 below, the agent is at the right edge of the environment and is attempting to move up to the cell marked T. The following outcomes are possible:</p>
</li>
<li><p>the agent moves up to enter cell T as intended with probability p;</p>
</li>
<li><p>the agent moves up and drifts left to enter cell L with probability (1 − p)&#x2F;2;</p>
</li>
<li><p>the agent agent remains in place with probability (1 − p)&#x2F;2 because moving up and drifting</p>
<p>right would take the agent out of bounds.</p>
<p>Figure 3 below shows how block cells influence outcomes. The agent is again in the cell marked X and attempting to move up. The only movement that can occur is moving up and drifting left (with probability (1−p)&#x2F;2); moving up without drift and moving up with rightward drift both lead to block cells which the agent cannot enter, so these outcomes result in no movement. (Note that the possibility of drift allows the agent to move on the diagonal in an indirect way; this can lead to some interesting “wall-hack” behavior that allows the agent to squeeze through gaps between two block cells that are adjacent to the agent’s current location.)</p>
</li>
</ul>
<p>As the agent moves, it receives rewards as follows:</p>
<p>• Entering a goal cell yields a reward of +10.<br> • Entering a mine cell yields a reward of −100.<br> • Entering a cliff cell yields a reward of −20.<br> • Acting in a cliff cell and returning to the start yields a reward of −10. • Any other result yields a reward of −1.</p>
<p>File Format</p>
<p>The environment for the agent will be specified in a text file containing cell markers (S, G, , B, M, C) as described earlier. Empty lines and lines that begin with # should be ignored. A simple 2 × 5 grid world is shown below:</p>
<p>M_BG_ CS__C</p>
<p>Several example input files are provided in the a04-data.zip archive on Canvas.</p>
<p>Basic Reinforcement Learning</p>
<p>The basic learning agent will use either the SARSA or Q-Learning algorithms to learn Q values for all state-action pairs via temporal difference updates. Your basic learning agent will need to maintain Q(s,a) values for all possible state-action pairs (s,a), where s is one of the cells in the grid world and a is one of the four movement directions (up, down, left, right).</p>
<p>To learn, the agent should perform T learning episodes (trials), where the value of T is either specified via the command-line flag -T or the default of 10,000. In each learning episode (trial), the agent begins in the start cell of the environment. The agent should then select and execute actions according to an ε-greedy policy and update its Q values using either on-policy (SARSA) or off-policy (Q-Learning) updates as appropriate. The agent should continue acting until it reaches a terminal state (goal or mine cell) or it takes a number of actions equal to the entire size (width × height) of the environment, at which point the episode ends.</p>
<p>Parameters and Decay</p>
<p>Initially, the agent should use the control parameters α, γ, and ε with values set to their defaults or specified via corresponding command-line arguments. As learning proceeds, the agent should periodically update its α and ε parameters to allow the Q values to converge and to direct the agent towards a pure greedy policy. Let α(0) and ε(0) denote the initial values of α and ε, and assume that the learning episodes performed by the agent are labeled 1, 2, . . . , T . In episode t, the agent should use α(t) and ε(t) values determined as</p>
<p> where Nα and Nε are (integer-valued) hyperparameters that control the decay of the learning rate</p>
<p>and policy randomness, respectively, and ⌈·⌉ is the ceiling function.<br> By design, the α(t) values will only change after Nα episodes have completed (similarly for the</p>
<p>ε(t) values). For example, we have</p>
<p>α(0) if t ∈ {1,2,…,Nα},<br> α(t) &#x3D;α(0)&#x2F;2 ift∈{Nα+1,Nα+2,…,2Nα},</p>
<p>α(0)&#x2F;3 ift∈{2Nα+1,…,3Nα}</p>
<p>CS 457&#x2F;557 Assignment 4 Page 5 of 12</p>
<p>This means that the agent only needs to update its α value after completing an episode that is a multiple of Nα (similarly for ε). With a little algebra, we can derive the following update rules, where t is the number of the episode that was just completed:</p>
<p>α(0)<br> • After every Nα episodes, update α to 1 + ⌊t&#x2F;Nα⌋.</p>
<p>ε(0)<br> • After every Nε episodes, update ε to 1 + ⌊t&#x2F;Nε⌋.</p>
<p>The periodic update rules above avoid having to recalculate α(t) and ε(t) at the start of each episode, and they also simplify the calculations somewhat (Hint: you can use integer division in Java to compute the floor function).</p>
<p>Evaluation of Final Learned Policy</p>
<p>After all learning episodes have completed, the agent should evaluate the performance of a pure greedy policy based on its learned Q values. To do this, the agent should perform 50 evaluation episodes; in each evaluation episode, the agent should always act greedily based on its Q values, and it should not update its Q values during evaluation. At the end of each evaluation episode, the agent should record the total reward (without discounting) that it earned by following the greedy policy. The performance of the greedy policy is then estimated as the average reward earned across the 50 evaluation episodes.</p>
<p>(Hint: There are only a few differences between learning episodes and evaluation episodes, so you should be able to write common code for performing an episode of either type, and customize behavior based on one or two conditional flags.)</p>
<p>Output</p>
<p>The level of output produced by the program is controlled by the -v flag (for verbosity). The levels range from 1 to 4, with 4 containing the most output; the output produced is additive, meaning that the output for level i should also be produced for any level j with j ≥ i. The desired output for each level is discussed in more detail below.</p>
<p>Output Level 1<br> Level 1 prints a brief summary of the program flow followed by details of the learned policy:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-example.txt -v 1 * Reading data&#x2F;simple-example.txt…<br> * Beginning 10000 learning episodes with SARSA…</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.500</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">MvBG&lt;</span><br><span class="line">&lt;&gt;&gt;ˆ&lt;</span><br></pre></td></tr></table></figure>

<p>Printing the policy involves displaying the best action (or actions in the case of ties) in each non- terminal and non-block state, where best is determined by the Q values, while displaying the cell marker for M, G, and B states. If you have a console that supports unicode output, then you can display the policy using unicode characters with the -u flag. The various combinations of best action and corresponding printed character (ASCII or Unicode) are shown below:</p>
<p>CS 457&#x2F;557</p>
<p>Assignment 4</p>
<p>Page 6 of 12</p>
<p>Best Action Set { Left }</p>
<p>{ Up } { Right } {Down}</p>
<p>{ Left, Right } { Up, Down } { Up, Left }<br> { Up, Right } { Down, Right } { Down, Left }</p>
<p>{ Up, Down, Right } { Up, Down, Left } { Down, Left, Right } { Up, Left, Right }</p>
<p>ASCII Symbol</p>
<p>&lt; ← ^ ↑ &gt; → v ↓</p>
<p>- ↔ | ↕ \ ↖ &#x2F; ↗ \ ↘ &#x2F; ↙</p>
<p>&gt; ⊢ &lt; ⊣ v ⊤ ^ ⊥</p>
<p>Unicode<br> Java character</p>
<p>\u2190 \u2191 \u2192 \u2193</p>
<p>\u2194 \u2195 \u2196 \u2197 \u2198 \u2199</p>
<p>\u22a2 \u22a3 \u22a4 \u22a5</p>
<p>{ Up, Down, Left, Right }<br> You can print a unicode character in Java using System.out.print(“\u2190”); or similar. An</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-f data/simple-minefield.txt -q -u -p 1</span><br></pre></td></tr></table></figure>

<p>is shown below:</p>
<p>Embedding unicode output in this pdf document directly is non-trivial, so all remaining examples here will use ASCII output.</p>
<p>+ (just use ASCII)<br> image of unicode output on the console, produced with the command-line arguments</p>
<p>CS 457&#x2F;557 Assignment 4 Page 7 of 12</p>
<p>Output Level 2<br> Level 2 includes the final learned Q values in the output:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-example.txt -v 2 -q * Reading data&#x2F;simple-example.txt…<br> * Beginning 10000 learning episodes with Q-Learning…</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.620</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">MvBG&lt;</span><br><span class="line">ˆ&gt;&gt;ˆv</span><br><span class="line">* Learned Q values:</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">|</span><br><span class="line">|0.0</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">0.0  |    -4.4  |</span><br><span class="line">     |-92.5     |0.0</span><br></pre></td></tr></table></figure>

<p>0.0 | |0.0</p>
<p>0.0 | |9.8</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  0.0|      -1.8|</span><br><span class="line">0.0  |     0.4  |</span><br></pre></td></tr></table></figure>

<p>0.0| 0.0 |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">           7.3  |</span><br><span class="line">                |</span><br><span class="line">             7.4|</span><br><span class="line">0.0  |   -23.5  |</span><br></pre></td></tr></table></figure>

<p>0.0|</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|    -5.1  |   -18.8  |     5.5  |</span><br><span class="line">|-5.3      |-33.9     |4.1       |6.1</span><br><span class="line">|      -5.4|       4.9|       7.3|</span><br><span class="line">|    -5.6  |     4.0  |     5.8  |</span><br><span class="line">--------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>Each cell (state) in the environment is displayed in the above output, along with the four Q values associated with the different actions in that state. Note that terminal cells and blocks should have all of their Q values set to 0, because the agent never acts in such cells. Each cell has an internal width of 10 characters (which excludes the borders). Each Q value is displayed with a width of 6 characters (leading zeroes are suppressed) and one decimal point (format specifier %6.1f) and the Q values are arranged as follows:</p>
<p>• Q(s,Up) and Q(s,Down) are centered at the top and bottom of the cell, respectively, each with a minimum of two characters of blank space on either side;</p>
<p>• Q(s, Left) is flushed left on the second line in the cell (use format specifier %-6.1f here); • Q(s, Right) is flushed right on the third line in the cell.</p>
<p>Your output doesn’t have to match this exactly, but it should be displayed in a clean and readable format; in particular, strive to ensure that the cells are displayed with a fixed width.</p>
<p>Output Level 3</p>
<p>Level 3 will periodically evaluate a pure greedy policy based on the current Q values at various stages during the learning process. Specifically, after every T&#x2F;10 learning episodes, the agent should perform 50 evaluation episodes and record the average total reward that is earned across these evaluation episodes. These evaluation episodes should operate as described earlier in the Evaluation of Final Learned Policy section. (Note that at this output level, it is fine to have your program evaluate the last greedy policy twice, once after the last learning episode and once at the end for final evaluation; depending on uncertainty within the environment, you might get different average total reward estimates for these separate evaluations.)</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simpler-example.txt -v 3 -T 1200 * Reading data&#x2F;simpler-example.txt…</p>
<p>9.4 | |-5.1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-5.0  |</span><br><span class="line">      |</span><br><span class="line">  -5.2|</span><br><span class="line">-4.9  |</span><br></pre></td></tr></table></figure>

<p>-20.2| 7.6 |</p>
<p>CS 457&#x2F;557 Assignment 4 Page 8 of 12</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">* Beginning 1200 learning episodes with SARSA...</span><br><span class="line">* After</span><br><span class="line">* Episode</span><br><span class="line">      120</span><br><span class="line">      240</span><br><span class="line">      360</span><br><span class="line">      480</span><br><span class="line">      600</span><br><span class="line">      720</span><br><span class="line">      840</span><br><span class="line">      960</span><br><span class="line">     1080</span><br><span class="line">     1200</span><br><span class="line">Avg. Total Reward for</span><br><span class="line">Current Greedy Policy</span><br><span class="line">  -8.000</span><br><span class="line">   3.580</span><br><span class="line">  -8.000</span><br><span class="line">  -8.000</span><br><span class="line">   3.020</span><br><span class="line">  -8.000</span><br><span class="line">   4.780</span><br><span class="line">  -6.860</span><br><span class="line">   6.760</span><br><span class="line">   6.960</span><br><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.060</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">v&gt;&gt;G</span><br><span class="line">&gt;&gt;&gt;ˆ</span><br><span class="line">* Learned Q values:</span><br><span class="line">---------------------------------------------</span><br><span class="line">                                       0.0  |</span><br><span class="line">                                            |</span><br><span class="line">                                         0.0|</span><br><span class="line">                                       0.0  |</span><br><span class="line">---------------------------------------------</span><br></pre></td></tr></table></figure>

<p>| -3.1 |</p>
<p>0.3 | |1.4</p>
<p>5.7 | |0.0</p>
<p>|-3.9 |<br> |</p>
<p>|-2.2 -4.8|</p>
<p>7.1| 0.3 |</p>
<p>10.0| 5.1 |</p>
<p>1.2 |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|</span><br><span class="line">|1.2</span><br><span class="line">|</span><br><span class="line">|</span><br><span class="line">---------------------------------------------</span><br></pre></td></tr></table></figure>

<p>Output Level 4<br> Level 4 displays the changes made to the α and ε parameters during the learning process:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-example.txt -v 4 -q -T 200 -na 100 -ne 50 * Reading data&#x2F;simple-example.txt…<br> * Beginning 200 learning episodes with Q-Learning…</p>
<p>-0.4 | |-0.2</p>
<p>3.4 | |0.2</p>
<p>3.4 | |4.1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">9.7  |</span><br><span class="line">     |</span><br><span class="line">  7.5|</span><br><span class="line">6.9  |</span><br></pre></td></tr></table></figure>

<p>4.7| 0.7 |</p>
<p>7.1| 3.3 |</p>
<p>7.8| 4.1 |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">* After</span><br><span class="line">* Episode</span><br><span class="line">       20</span><br><span class="line">       40</span><br><span class="line">Avg. Total Reward for</span><br><span class="line">Current Greedy Policy</span><br></pre></td></tr></table></figure>

<p>-4.720 7.340</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(after episode 50, epsilon to 0.45000)</span><br></pre></td></tr></table></figure>

<p>60</p>
<p>80 100</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  7.620</span><br><span class="line">-10.000</span><br><span class="line">  0.340</span><br><span class="line">(after episode 100, alpha to 0.45000)</span><br><span class="line">(after episode 100, epsilon to 0.30000)</span><br></pre></td></tr></table></figure>

<p>120 7.440</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    140      7.400</span><br><span class="line">(after episode 150, epsilon to 0.22500)</span><br><span class="line">    160      7.440</span><br><span class="line">    180      7.260</span><br><span class="line">    200      7.700</span><br></pre></td></tr></table></figure>

<p>CS 457&#x2F;557 Assignment 4 Page 9 of 12</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    (after episode 200, alpha to 0.30000)</span><br><span class="line">    (after episode 200, epsilon to 0.18000)</span><br><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.560</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">MvBG&lt;</span><br><span class="line">ˆ&gt;&gt;ˆ&gt;</span><br><span class="line">* Learned Q values:</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">|     0.0  |    -1.9  |</span><br><span class="line">|0.0       |-100.0    |0.0</span><br><span class="line">|       0.0|      -1.4|</span><br><span class="line">|     0.0  |     4.3  |</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">|    -3.7  |   -58.8  |     5.9  |</span><br><span class="line">|-7.1      |-52.6     |4.1       |6.3</span><br><span class="line">|      -4.2|       6.0|       7.4|</span><br><span class="line">|    -5.1  |     4.5  |     6.5  |</span><br><span class="line">--------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>0.0 | |0.0</p>
<p>0.0 | |9.8</p>
<p>0.0| 0.0 |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">           2.6  |</span><br><span class="line">                |</span><br><span class="line">             6.5|</span><br><span class="line">0.0  |   -22.2  |</span><br></pre></td></tr></table></figure>

<p>0.0|</p>
<p>9.4 | |-4.7</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-5.0  |</span><br><span class="line">      |</span><br><span class="line">  -4.6|</span><br><span class="line">-5.1  |</span><br></pre></td></tr></table></figure>

<p>-18.9| 7.5 |</p>
<p>CS 457&#x2F;557 Assignment 4 Page 10 of 12</p>
<p>Extra Credit Options</p>
<p>Eligibility Traces</p>
<p>The optional -l command-line argument allows for specification of the λ parameter to be used in conjunction with eligibility traces. For λ &#x3D; 0, the learning agent should perform temporal difference updates as before (i.e., only one Q(s, a) value should be updated at each step in a learning episode).</p>
<p>For λ &gt; 0, the agent will need to store an eligibility value e(s, a) for each (s, a) ∈ S × A. Whenever the agent takes action a in state s, e(s, a) should go up by 1. After each step in a learning episode, the agent should update the Q and e values for each state-action pair (s′′, a′′) ∈ S × A using:</p>
<p>Q(s′′, a′′) ← Q(s′′, a′′) + αδe(s′′, a′′), e(s′′, a′′) ← γλe(s′′, a′′),</p>
<p>where δ is the TD error at the current step. Some additional notes:</p>
<p>• The eligibility values should be reset back to 0 at the start of each episode.</p>
<p>• Using eligibility traces in conjunction with SARSA is straightforward, but using traces in Q-Learning requires more care. Specifically, in Q-Learning, the eligibility values e(s, a) need to be reset back to 0 whenever the next action is sub-optimal.</p>
<p>See Lecture 09-3 for further details.<br> The example below uses SARSA with 100 learning episodes and deterministic movement (i.e., every</p>
<p>action succeeds as intended), without eligibility traces:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-cliff.txt -p 1 -T 100 -l 0.0 * Reading data&#x2F;simple-cliff.txt…<br> * Beginning 100 learning episodes with SARSA…</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: -8.000</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">&lt;v&gt;G</span><br><span class="line">v&gt;&gt;ˆ</span><br></pre></td></tr></table></figure>

<p>When eligibility traces are included, learning improves:</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-cliff.txt -p 1 -T 100 -l 0.3 * Reading data&#x2F;simple-cliff.txt…<br> * Beginning 100 learning episodes with SARSA…</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.000</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">v&lt;&lt;G</span><br><span class="line">&gt;&gt;&gt;ˆ</span><br></pre></td></tr></table></figure>

<p>CS 457&#x2F;557 Assignment 4 Page 11 of 12</p>
<p>Feature-Based Q-Learning</p>
<p>The optional -w command-line argument should toggle the use of feature-based Q-learning. Instead of maintaining Q values for each state-action pair in a table, the agent should maintain a set of weights associated with features that characterize the state-action pairs.</p>
<p>For any state s, we will define the following features:</p>
<p>f0(s) &#x3D; 1<br> f1(s) &#x3D; x coordinate of s</p>
<p>xmax<br> f2(s) &#x3D; y coordinate of s</p>
<p>ymax<br> f3(s) &#x3D; 1 if s is a mine cell, 0 otherwise</p>
<p>f4(s) &#x3D; 1 if s is a cliff cell, 0 otherwise f5(s) &#x3D; 1 if s is a goal cell, 0 otherwise f6(s) &#x3D; L1 distance from s to nearest goal</p>
<p>xmax + ymax</p>
<p>where we assume that the x and y coordinates of states are positive integers, (1, 1) is the state in the lower left corner of the environment, and (xmax,ymax) is the state in the upper right corner of the environment; i.e., state coordinates look like:</p>
<p>(xmax, ymax)</p>
<p>For any state-action pair (s, a), we will define the following features, most of which are based on the features of the likely next state s′ that would result from taking action a in state s (i.e., s′ is the state that the agent would reach if it did not drift):</p>
<p>fj(s,a) &#x3D; fj(s′) for j ∈ {0,1,…,6} f7(s,a) &#x3D; 1 if s &#x3D; s′, 0 otherwise</p>
<p>The agent should maintain a weight wj for each state-action feature fj, and it should estimate the Q value for any (s, a) pair as</p>
<p>7</p>
<p>Q(s,a)&#x3D;Xwj ·fj(s,a) j&#x3D;0</p>
<p>These weights should be initialized to 1 and then updated during learning episodes as described in Lecture 09-4, where δ is the TD error:</p>
<p>wj←wj+αδfj(s,a) ∀j∈{0,1,2,…,7}</p>
<p>For verbosity level 2 and up, your program should print out the final learned weights after the Q values (note that these Q values should be the estimated values based on the weights and state- action features). An example is shown on the next page.</p>
<p>(1, ymax)</p>
<p>.</p>
<p>(1, 1)</p>
<p>(2, 1)</p>
<p>…</p>
<p>(xmax, 1)</p>
<p>CS 457&#x2F;557 Assignment 4 Page 12 of 12</p>
<p><strong>shell</strong><strong>$</strong> java Driver -f data&#x2F;simple-cliff.txt -q -p 1 -T 50 -v 2 -w * Reading data&#x2F;simple-cliff.txt…<br> * Beginning 50 learning episodes with Q-Learning…</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  Done with learning!</span><br><span class="line">* Beginning 50 evaluation episodes...</span><br><span class="line">  Avg. Total Reward of Learned Policy: 7.000</span><br><span class="line">* Learned greedy policy:</span><br><span class="line">&gt;v+G</span><br><span class="line">&gt;&gt;&gt;ˆ</span><br><span class="line">* Learned Q values:</span><br><span class="line">---------------------------------------------</span><br><span class="line">|    50.3  |    58.5  |    57.2  |    95.3  |</span><br><span class="line">|50.3      |50.6      |57.2      |32.3      |</span><br><span class="line">|      58.9|      32.3|      57.2|</span><br><span class="line">|    57.2  |    65.4  |    57.2  |</span><br><span class="line">---------------------------------------------</span><br><span class="line">|    50.6  |    58.9  |    32.3  |    95.7  |</span><br><span class="line">|56.9      |57.2      |65.4      |73.7      |</span><br></pre></td></tr></table></figure>

<p>95.3| 81.9 |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">|      65.4|      73.7|      81.9|</span><br><span class="line">|    56.9  |    65.1  |    73.3  |</span><br><span class="line">---------------------------------------------</span><br><span class="line">* Learned weights</span><br><span class="line">  Feature 0 (constant):  35.526</span><br><span class="line">  Feature 1 (scaled-X):  33.358</span><br><span class="line">  Feature 2 (scaled-Y):  12.902</span><br><span class="line">  Feature 3 (   ind-M):   1.000</span><br><span class="line">  Feature 4 (   ind-C): -34.795</span><br><span class="line">  Feature 5 (   ind-G):  20.321</span><br><span class="line">  Feature 6 (  dist-G):   0.658</span><br><span class="line">  Feature 7 (ind-move):  -0.339</span><br></pre></td></tr></table></figure>

<p>Some final notes:</p>
<ul>
<li>In my limited experimentation with feature-based learning, the learning process seems a bit more temperamental with regards to the various control parameters (e.g., α, ε, decay rates). Q-Learning with features tends to do better than SARSA with features, but you can probably get SARSA to behave well by adjusting the control parameters appropriately (e.g., extending the learning process, adjusting ε decay).</li>
<li>If you’re feeling adventurous, you can try to devise some additional features to use and see if they work better than what is given here. You could also try mixing and matching the features, e.g., perhaps by removing the coordinate-based features and replacing them with something else. Let me know if you find any features that seem to work well!</li>
<li>You can also add eligibility traces to feature-based learning (i.e., support both the -l and -w flags at the same time). This requires a few modifications to the eligibility values and the weight update process, though. You can read up on this in the RL textbook and&#x2F;or stop by office hours if you’re curious to know how this might be done.</li>
</ul>
<p>81.6| 81.6 |</p>

    </div>

    
    
    <div class="post-widgets">
      <div id="needsharebutton-postbottom">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    </div>
        <div class="reward-container">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>微信扫码留学生CS作业
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://javamianshi.vercel.app/posts/48125.html" title="CS 457&#x2F;557 Assignment 4 Due December 11 2024 by 11:00 pm (Central)">https://javamianshi.vercel.app/posts/48125.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
<div class="wechat_channel">
  <br>
  <!-- 这里添加你的二维码图片 -->
  <!-- <img src ="/images/qq3.png"> -->
  <img src ="/images/avator.jpg">
</div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CS-457-557-Assignment-4-Due-December-11-2024-by-11-00-pm-Central/" rel="tag"># CS 457/557 Assignment 4 Due December 11 2024 by 11:00 pm (Central)</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/35542.html" rel="prev" title="留学生Computer Science编程作业代写、留学生CS辅导、CS编写">
      <i class="fa fa-chevron-left"></i> 留学生Computer Science编程作业代写、留学生CS辅导、CS编写
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="微信扫码留学生CS作业"
      src="/images/avator.jpg">
  <p class="site-author-name" itemprop="name">微信扫码留学生CS作业</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3lvdXJuYW1l" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname"><i class="fa fa-fw fa-github"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOjQxMTM3MjY5OEBxcS5jb20=" title="E-Mail → mailto:411372698@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>
  <div class="">
    <a target="_blank" class="social-link" href="/atom.xml" style="color: burlywood;">
      <span class="icon">
        <i class="fa fa-rss"></i>
      </span>
      <span class="label">RSS</span>
    </a>
  </div>


      </div>
      <div class="wechat_channel">
        <br>
        <!-- 这里添加你的二维码图片 -->
       <!-- <img src ="/images/qq3.png"> -->
        <!-- <span>公众号</span> -->
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">微信扫码留学生CS作业</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">27k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">24 分钟</span>
</div>
<!--
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZw==">NexT.Gemini</span> 强力驱动
  </div>
-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>













  

  

  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js"></script>
  <script>
      pbOptions = {};
        pbOptions.iconStyle = "box";
        pbOptions.boxForm = "horizontal";
        pbOptions.position = "bottomCenter";
        pbOptions.networks = "Wechat,QQZone,Weibo,Douban,Twitter,Facebook";
      new needShareButton('#needsharebutton-postbottom', pbOptions);
  </script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true}});</script></body>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/clicklove.js"></script>
</html>
